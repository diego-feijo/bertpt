{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pre-training ALBERT from Wikipedia using TPU",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/diego-feijo/bertpt/blob/master/Pre_training_ALBERT_from_Wikipedia_using_TPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2XB_l-Hgzq_",
        "colab_type": "text"
      },
      "source": [
        "# Pre-training ALBERT from Wikipedia using TPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZPgpRl5g2e2",
        "colab_type": "text"
      },
      "source": [
        "This kernel shows how to pre-train an [ALBERT](https://github.com/google-research/ALBERT) model from Wikipedia dump using free Colab TPU v2.\n",
        "\n",
        "These are the steps to follow:\n",
        "\n",
        "1. Setting Up the Environment\n",
        "2. Download and Prepare Data\n",
        "3. Extract Raw Text\n",
        "4. Build SentencePiece Model\n",
        "5. Generate pre-training data\n",
        "6. Train the model\n",
        "\n",
        "Colab TPU requires a [Google Cloud Storage bucket](https://cloud.google.com/tpu/docs/quickstart). New users have [$300 free credit](https://cloud.google.com/free/) for one year. \n",
        "\n",
        "After each step, we save persistent data so we can always stop and resume from the last finished step. If you need to resume, run the first step again and then go straight to the step you need to continue.\n",
        "\n",
        "**Notes** \n",
        "\n",
        "1. You **need** to set the same BUCKET_NAME in steps 3-6.\n",
        "2. Steps 5 and 6 can take several hours to run. Google Colab will interrupt after 8 hour running, so it will be necessary to start over from the previous finished step.\n",
        "\n",
        "**Credits**: This tutorial was adapted from https://towardsdatascience.com/pre-training-bert-from-scratch-with-cloud-tpu-6e2f71028379"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODimOhBR05yR",
        "colab_type": "text"
      },
      "source": [
        "MIT License\n",
        "\n",
        "Copyright (c) [2019] [Diego de Vargas Feijo]\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjad5jsr9YaM",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Setting Up the Environment\n",
        "Install dependencies, import globally required packages and authorize with Google Account to access Colab TPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S4CiOh3RzFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade -q sentencepiece\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import nltk\n",
        "import os\n",
        "#import random\n",
        "import sentencepiece as spm\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "#from glob import glob\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()\n",
        "  \n",
        "# configure logging\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "# create formatter and add it to the handlers\n",
        "formatter = logging.Formatter('%(asctime)s:  %(message)s')\n",
        "sh = logging.StreamHandler()\n",
        "sh.setLevel(logging.INFO)\n",
        "sh.setFormatter(formatter)\n",
        "log.handlers = [sh]\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  log.info(\"Using TPU runtime\")\n",
        "  USE_TPU = True\n",
        "  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "  with tf.Session(TPU_ADDRESS) as session:\n",
        "    log.info('TPU address is ' + TPU_ADDRESS)\n",
        "    # Upload credentials to TPU.\n",
        "    with open('/content/adc.json', 'r') as f:\n",
        "      auth_info = json.load(f)\n",
        "    tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "    \n",
        "else:\n",
        "  log.warning('Not connected to TPU runtime')\n",
        "  USE_TPU = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j532CiG7iQSx",
        "colab_type": "text"
      },
      "source": [
        "Clone and Patch ALBERT sources\n",
        "\n",
        "Some Albert sources use deprecated API that generate a lot of warnings. We also make some minor changes to the scripts can run smoothly on Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b3TXHowYu-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone the repository\n",
        "!test -d ALBERT || git clone https://github.com/google-research/ALBERT.git ALBERT\n",
        "\n",
        "# Avoid deprecated warnings\n",
        "!sed -i 's/tf.logging/tf.compat.v1.logging/' ALBERT/*.py\n",
        "!sed -i 's/tf.app.run/tf.compat.v1.app.run/' ALBERT/*.py\n",
        "\n",
        "# Avoid error when the line contains only one number\n",
        "!sed -i 's/i.lower()/str(i).lower()/' ALBERT/create_pretraining_data.py\n",
        "\n",
        "# Create Dummy flag (Colab Bug)\n",
        "!sed -i 's/FLAGS = flags.FLAGS/FLAGS=flags.FLAGS\\n\\nflags.DEFINE_string(\"f\", \"\", \"Dummy flag. Not used.\")/' ALBERT/run_pretraining.py\n",
        "\n",
        "# Mute too verbose output\n",
        "!sed -i 's/tf.compat.v1.logging.info/# tf.compat.v1.logging.info/' ALBERT/tokenization.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptsoXM_qgtms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not 'ALBERT' in sys.path:\n",
        "  sys.path += ['ALBERT']\n",
        "\n",
        "import modeling, optimization, tokenization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGVXMoC-aMy1",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Download and Prepare Data\n",
        "\n",
        "Wikipedia dump is available in XML format. We need to extract the raw text from it. It is possible to use any language, I listed here the most frequent ones ([sorted by the number of active users](https://en.wikipedia.org/wiki/List_of_Wikipedias)):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0PSBfMurgxP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LANG = \"pt\"  #@param ['en', 'fr', 'de', 'es', 'ja', 'ru', 'it', 'zh', 'pt', 'ar', 'fa', 'pl', 'nl', 'id', 'uk', 'he', 'sv', 'cs', 'ko', 'vi', 'ca', 'no', 'fi', 'hu', 'th', 'el', 'hi', 'bn', 'ceb', 'tr', 'ro', 'sw', 'kk', 'da', 'eo', 'sr', 'lt', 'sk', 'bg', 'min', 'sl', 'eu', 'et', 'hr', 'te', 'nn', 'gl']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxW6d4o0dN4j",
        "colab_type": "text"
      },
      "source": [
        "The latest Wikipedia dump can be from the day 1 or 20, but the date when dump is finished can vary. Instead of complicated inspecting in the page, we are guessing when the dump is ready."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHZU9byCnj72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "\n",
        "def get_last_dump():\n",
        "  today = datetime.datetime.now()\n",
        "\n",
        "  if today.day > 8 and today.day < 25:\n",
        "    day = 1\n",
        "    month = today.month\n",
        "  elif today.day >= 25:\n",
        "    day = 20\n",
        "    month = today.month\n",
        "  else:\n",
        "    day = 1\n",
        "    month = today.month - 1\n",
        "  return '{}{:02d}{:02d}'.format(today.year, month, day)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEOzdsF9k5uH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "last_dump = get_last_dump()\n",
        "corpus = tf.keras.utils.get_file(\n",
        "    \"{}wiki.bz2\".format(LANG),\n",
        "    \"https://dumps.wikimedia.org/{}wiki/{}/{}wiki-{}-pages-articles-multistream.xml.bz2\".format(\n",
        "        LANG,\n",
        "        last_dump,\n",
        "        LANG,\n",
        "        last_dump\n",
        "    ))\n",
        "!bzip2 -d {corpus}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0c7tasirPFL",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Extract Raw Text\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7fU7ElOa94t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUCKET_NAME = \"<Insert Bucket Name Here>\" # @param string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-gu9uDbbAIs",
        "colab_type": "text"
      },
      "source": [
        "Uses WikiExtractor to remove XML tags and keep only raw text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLaXliDWldgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!test -d wikiextractor || git clone https://github.com/attardi/wikiextractor.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEDAwhI5lzgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WIKI_INPUT_FILE, _ = os.path.splitext(corpus)\n",
        "WIKI_EXTRACTED_DIR = \"wikimedia\" \n",
        "\n",
        "tf.io.gfile.makedirs(WIKI_EXTRACTED_DIR)\n",
        "!python3 wikiextractor/WikiExtractor.py -q -c -o {WIKI_EXTRACTED_DIR} {WIKI_INPUT_FILE}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzgKtLGjEIYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Snowball Stemmers\n",
        "LANG_STM = \"portuguese\" # @param ['danish', 'english', 'finnish', 'french', 'german', 'hugarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish']\n",
        "\n",
        "sent_tokenizer = nltk.data.load('tokenizers/punkt/{}.pickle'.format(LANG_STM))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY-Kvnx6sUFS",
        "colab_type": "text"
      },
      "source": [
        "Prepare input for create_pretraining_data script.\n",
        "\n",
        "Input file format requires:\n",
        "- One sentence per line. These should ideally be actual sentences, not entire paragraphs or arbitrary spans of text. (Because we use the sentence boundaries for the \"next sentence prediction\" task).\n",
        "- Blank lines between documents. Document boundaries are needed so that the \"next sentence prediction\" task doesn't span between documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr9Rau_WEbpn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bz2\n",
        "\n",
        "PRC_DATA_FPATH = \"proc_wikimedia.txt\" #@param {type: \"string\"}\n",
        "\n",
        "with open(PRC_DATA_FPATH, \"w\", encoding=\"utf-8\") as fo:\n",
        "  for group in os.listdir(WIKI_EXTRACTED_DIR):\n",
        "    basedir = os.path.join(WIKI_EXTRACTED_DIR, group)\n",
        "    for fz in os.listdir(basedir):\n",
        "      if fz.endswith('.bz2'):\n",
        "        with bz2.BZ2File(os.path.join(basedir, fz), 'r') as fi:\n",
        "          contents = fi.read()\n",
        "        is_title = False\n",
        "        text = contents.decode('utf-8')\n",
        "        for l in text.splitlines():\n",
        "          # print (l)\n",
        "          if l.startswith('</doc>'):\n",
        "            # Empty line for each new document\n",
        "            fo.write(\"\\n\")\n",
        "          elif len(l) == 1:\n",
        "            # Empty lines must be ignored\n",
        "            pass\n",
        "          elif l.startswith('<doc'):\n",
        "            # After the heading, there is a title\n",
        "            is_title = True\n",
        "          elif is_title:\n",
        "            # Ignore this line, reset variable\n",
        "            is_title = False\n",
        "          else:\n",
        "            # Wikipedia uses multiple sentences in on line\n",
        "            # We need to split one sentence per line\n",
        "            sentences = sent_tokenizer.tokenize(l)\n",
        "            for sentence in sentences:\n",
        "              fo.write(sentence + \"\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FfBamX6BHfDD",
        "colab": {}
      },
      "source": [
        "!head {PRC_DATA_FPATH}\n",
        "!test -f {PRC_DATA_FPATH}.gz || gzip < {PRC_DATA_FPATH} > {PRC_DATA_FPATH}.gz\n",
        "tf.gfile.MakeDirs(\"gs://{}/datasets/\".format(BUCKET_NAME))\n",
        "!gsutil -m cp {PRC_DATA_FPATH}.gz gs://{BUCKET_NAME}/datasets/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVO9EnUwrluQ",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Build SentencePiece Model\n",
        "In this step we will be generating the config files and the encoder to covert text to integers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUPhcVtlT_Hb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUCKET_NAME = \"<Insert Bucket Name Here>\" # @param {type: \"string\"}\n",
        "\n",
        "MODEL_DIR = \"albert_cased_L-12_H-768_A-12\" #@param {type: \"string\"}\n",
        "VOC_SIZE = 30000 #@param {type:\"integer\"}\n",
        "\n",
        "!test -f {PRC_DATA_FPATH}.gz || gsutil -m cp gs://{BUCKET_NAME}/datasets/{PRC_DATA_FPATH}.gz .\n",
        "!test -f {PRC_DATA_FPATH} || gzip -d < {PRC_DATA_FPATH}.gz > {PRC_DATA_FPATH}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YigCcV-hSHVH",
        "colab_type": "text"
      },
      "source": [
        "Build ALBERT Configuration Base model:\n",
        "- Base Model: https://tfhub.dev/google/albert_base/2\n",
        "- Large Model: https://tfhub.dev/google/albert_large/2\n",
        "- X-Large Model: https://tfhub.dev/google/albert_xlarge/2\n",
        "- XX-Large Model: https://tfhub.dev/google/albert_xxlarge/2\n",
        "\n",
        "It is not feasible to create pre-training data for models bigger than Large using Colab. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEpSGpUKReKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use this for ALBERT-base\n",
        "albert_config = {\n",
        "  \"attention_probs_dropout_prob\": 0.1, \n",
        "  \"hidden_act\": \"gelu\", \n",
        "  \"hidden_dropout_prob\": 0.1, \n",
        "  \"embedding_size\": 128,\n",
        "  \"hidden_size\": 768, \n",
        "  \"initializer_range\": 0.02, \n",
        "  \"intermediate_size\": 3072, \n",
        "  \"max_position_embeddings\": 512, \n",
        "  \"num_attention_heads\": 12,\n",
        "  \"num_hidden_layers\": 12,\n",
        "  \"num_hidden_groups\": 1,\n",
        "  \"net_structure_type\": 0,\n",
        "  \"gap_size\": 0, \n",
        "  \"num_memory_blocks\": 0, \n",
        "  \"inner_group_num\": 1,\n",
        "  \"down_scale_factor\": 1,\n",
        "  \"type_vocab_size\": 2,\n",
        "  \"vocab_size\": VOC_SIZE\n",
        "}\n",
        "\n",
        "with open(\"albert_config.json\", \"w\") as fo:\n",
        "  json.dump(albert_config, fo, indent=2)\n",
        "!gsutil -m cp albert_config.json gs://{BUCKET_NAME}/{MODEL_DIR}/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8WA6AmImJeK",
        "colab_type": "text"
      },
      "source": [
        "[SentencePiece](https://github.com/google/sentencepiece) will be used to encode the text.\n",
        "\n",
        "We need to train the SentencePiece model and build our vocabulary. It is required a lot of RAM. Even 35GB RAM offered by Colab may not be enough if all the raw text is used. We use SUBSAMPLE_SIZE to control how much memory is used.\n",
        "\n",
        "In case of Out of Memory, it is possible to reduce SUBSAMPLE_SIZE.\n",
        "\n",
        "The VOC_SIZE used by monolingual BERT and ALBERT papers are 30000. The multilingual uses 129000 tokens. It is not clear if increasing the VOC_SIZE will improve the model.\n",
        "\n",
        "NUM_PLACEHOLDERS can be used after the pre-training during the fine-tunning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doUwCk-oT4Em",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PRC_DATA_FPATH = \"proc_wikimedia.txt\"  #@param {type: \"string\"}\n",
        "MODEL_PREFIX = 'tokenizer' #@param {type:\"string\"}\n",
        "SUBSAMPLE_SIZE = 10000000 #@param {type:\"integer\"}\n",
        "# Number of reserved tokens at end of vocabulary\n",
        "# This should only be used when training data contains a small but very\n",
        "# frequent tokens.\n",
        "NUM_PLACEHOLDERS = 0 #@param {type:\"integer\"}\n",
        "\n",
        "SPM_COMMAND = ('--input={} --model_prefix={} '\n",
        "               '--vocab_size={} --input_sentence_size={} '\n",
        "               '--shuffle_input_sentence=true ' \n",
        "               '--pad_piece=[PAD] '\n",
        "               '--unk_piece=[UNK] '\n",
        "               '--pad_id=0 --unk_id=1 --user_defined_symbols=[CLS],[SEP],[MASK] ' \n",
        "               '--bos_id=-1 --eos_id=-1 ').format(\n",
        "               PRC_DATA_FPATH, MODEL_PREFIX, \n",
        "               VOC_SIZE - NUM_PLACEHOLDERS, SUBSAMPLE_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh9yz4DMj1fr",
        "colab_type": "text"
      },
      "source": [
        "Training SentencePiece may take several minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18nn6eW_s-fV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spm.SentencePieceTrainer.Train(SPM_COMMAND)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzahx55mFwmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!test -f {MODEL_PREFIX}.tar.gz && rm {MODEL_PREFIX}.tar.gz\n",
        "!tar czvf {MODEL_PREFIX}.tar.gz {MODEL_PREFIX}.*\n",
        "tf.gfile.MakeDirs(\"gs://{}/{}/\".format(BUCKET_NAME, MODEL_DIR))\n",
        "!gsutil -m cp {MODEL_PREFIX}.tar.gz gs://{BUCKET_NAME}/{MODEL_DIR}/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYlBqiv5UD-j",
        "colab_type": "text"
      },
      "source": [
        "Let's see the first tokens from the vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDJ9QmNMUEQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOC_FNAME = \"{}.vocab\".format(MODEL_PREFIX)\n",
        "MDL_FNAME = \"{}.model\".format(MODEL_PREFIX)\n",
        "\n",
        "!head {VOC_FNAME}\n",
        "!wc -l {VOC_FNAME}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MqXZnc3FCuY",
        "colab_type": "text"
      },
      "source": [
        "Let's check how SentencePiece tokenize one sentence. You may change to sentence in the language you are using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsSOnEnC-jG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testcase = \"[CLS] [MASK] Sentença de mérito. [SEP] Embargos de declaração 普通话.[SEP]\"\n",
        "\n",
        "bert_tokenizer = tokenization.FullTokenizer(VOC_FNAME, do_lower_case=False, spm_model_file=MDL_FNAME)\n",
        "tokens = bert_tokenizer.tokenize(testcase)\n",
        "ids = bert_tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(tokens)\n",
        "print(ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DauD8ndhEA-z",
        "colab_type": "text"
      },
      "source": [
        "Everything is working as expected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwFtStCo__QX",
        "colab_type": "text"
      },
      "source": [
        "## Step 5: Generate Pre-training Data\n",
        "Pre-training data is a collection of tfrecord (binary files where the text is SentencePiece encoded, and masking and auxiliary vectors were build).\n",
        "\n",
        "So, a text:\n",
        "\n",
        "- \"This is just one of many samples. This a sentence that follows. \" \n",
        "\n",
        "would be converted to:\n",
        "- \"2 1700 23 45 ... 67 89 ... 0 0 0\"\n",
        "\n",
        "\n",
        "The create_pretraining_data will insert special tokens ('\\[CLS\\]', '\\[SEP\\]', '\\[UNK\\]', '\\[PAD\\]')."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHWl4SD6boUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUCKET_NAME = \"<Insert Bucket Name Here>\" # @param {type: \"string\"}\n",
        "\n",
        "MODEL_DIR = 'albert_cased_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "MODEL_PREFIX = 'tokenizer' #@param {type:\"string\"}\n",
        "VOC_FNAME = \"{}.vocab\".format(MODEL_PREFIX)\n",
        "MDL_FNAME = \"{}.model\".format(MODEL_PREFIX)\n",
        "PRC_DATA_FPATH = \"proc_wikimedia.txt\" #@param {type:\"string\"}\n",
        "\n",
        "!test -f {PRC_DATA_FPATH}.gz || gsutil -m cp gs://{BUCKET_NAME}/datasets/{PRC_DATA_FPATH}.gz .\n",
        "!test -f {PRC_DATA_FPATH} || gzip -d < {PRC_DATA_FPATH}.gz > {PRC_DATA_FPATH}\n",
        "!test -f {MODEL_PREFIX}.tar.gz || gsutil -m cp gs://{BUCKET_NAME}/{MODEL_DIR}/{MODEL_PREFIX}.tar.gz .\n",
        "!test -f {VOC_FNAME} || tar xzvf {MODEL_PREFIX}.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzIuGpA5rxDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEMO_MODE = True # @param {type: \"boolean\"}\n",
        "\n",
        "# Reduce the number of lines to train faster\n",
        "if DEMO_MODE:\n",
        "  !head -1000000 {PRC_DATA_FPATH} > {PRC_DATA_FPATH}.tmp\n",
        "  !mv {PRC_DATA_FPATH}.tmp {PRC_DATA_FPATH}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZiOSjK7iqWh",
        "colab_type": "text"
      },
      "source": [
        "Since our corpus can be large, we will split it into shards:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyN1nI04-uKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf ./shards\n",
        "!mkdir ./shards\n",
        "!split -a 4 -l 256000 -d {PRC_DATA_FPATH} ./shards/shard_\n",
        "!ls ./shards/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-FSq3zNFvLs",
        "colab_type": "text"
      },
      "source": [
        "The **MAX_SEQ_LENGTH** (maximum sequence length) supported for the model is 512, but training time will be a lot slower because the complexity is quadratic to the length of sentences. Albert authors trained 90% of time using length 128 and the remaining using 512.\n",
        "\n",
        "To simulate this behaviour, it is necessary to create training using length 128,and then create pre-training data again using 512 and change the configuration file.\n",
        "\n",
        "The **DUPE_FACTOR** defines how many times each sequence will be used. Each sequence is randomly masked so it is a good use of the data to have as many duplicates as possible. However, using values larger than 20 may generate files larger than 1GB per shard. Larger files will not make the pre-training to run slowly, but will require a lot of space.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnZcD0yIBGPd",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"}\n",
        "MASKED_LM_PROB = 0.15 #@param {type: \"number\"}\n",
        "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
        "# Strip diacritics and Lowercase\n",
        "DO_LOWER_CASE = False #@param {type:\"boolean\"}\n",
        "DO_WHOLE_WORD_MASK = True #@param {type:\"boolean\"}\n",
        "PROCESSES = 2 #@param {type:\"integer\"}\n",
        "PRETRAINING_DIR = \"gs://{}/{}/pretraining_data_{}\".format(BUCKET_NAME, MODEL_DIR, MAX_SEQ_LENGTH)\n",
        "DUPE_FACTOR = 4 #@param {type:\"integer\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VAerF6pdxSD",
        "colab_type": "text"
      },
      "source": [
        "Confirm where the training files will be written."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMv84ixhtyEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PRETRAINING_DIR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-MibOBkFam2",
        "colab_type": "text"
      },
      "source": [
        "Let's try using as many cores are available. For each shard we need to call *create_pretraining_data.py* script. To that end, we will employ the  *xargs* command. \n",
        "\n",
        "This step will take a while to run. We will be saving generated data from each shards in the permanent storage.\n",
        "\n",
        "If you need to resume this step, you can check the bucket for generated files and manually delete the local shards that were already generated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rl6VDAdRA4vT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "XARGS_CMD = ('ls ./shards | '\n",
        "      'xargs -n 1 -P {} -I{} '\n",
        "      'python3 ALBERT/create_pretraining_data.py '\n",
        "      '--input_file=./shards/{} '\n",
        "      '--output_file={}/{}.tfrecord '\n",
        "      '--vocab_file={} '\n",
        "      '--spm_model_file={} '\n",
        "      '--do_lower_case={} '\n",
        "      '--do_whole_word_mask={} '\n",
        "      '--max_predictions_per_seq={} '\n",
        "      '--max_seq_length={} '\n",
        "      '--masked_lm_prob={} '\n",
        "      '--dupe_factor={} ')\n",
        "XARGS_CMD = XARGS_CMD.format(PROCESSES, '{}', '{}',\n",
        "                             PRETRAINING_DIR, '{}', \n",
        "                             VOC_FNAME, MDL_FNAME, DO_LOWER_CASE,\n",
        "                             DO_WHOLE_WORD_MASK, MAX_PREDICTIONS, \n",
        "                             MAX_SEQ_LENGTH, MASKED_LM_PROB, DUPE_FACTOR)\n",
        "!$XARGS_CMD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gdQEOzhYmSh",
        "colab_type": "text"
      },
      "source": [
        "## Step 6: Training the Model\n",
        "\n",
        "If you need to resume from an interrupted training, you may skip steps 2-5 and proceed from here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXAuzsJfYrio",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "BUCKET_NAME = \"<Insert Bucket Name Here>\" # @param {type: \"string\"}\n",
        "\n",
        "MODEL_DIR = 'albert_cased_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "\n",
        "# Input data pipeline config\n",
        "TRAIN_BATCH_SIZE = 256 #@param {type:\"integer\"}\n",
        "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
        "MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"}\n",
        "MASKED_LM_PROB = 0.15 #@param\n",
        "\n",
        "PRETRAINING_DIR = \"pretraining_data_{}\".format(MAX_SEQ_LENGTH)\n",
        "\n",
        "# Training procedure config\n",
        "EVAL_BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.00176\n",
        "TRAIN_STEPS = 175000 #@param {type:\"integer\"}\n",
        "SAVE_CHECKPOINTS_STEPS = 5000 #@param {type:\"integer\"}\n",
        "NUM_TPU_CORES = 8\n",
        "\n",
        "if BUCKET_NAME:\n",
        "  BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)\n",
        "else:\n",
        "  BUCKET_PATH = \".\"\n",
        "\n",
        "ALBERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR)\n",
        "DATA_GCS_DIR = \"{}/{}\".format(ALBERT_GCS_DIR, PRETRAINING_DIR)\n",
        "\n",
        "CONFIG_FILE = os.path.join(ALBERT_GCS_DIR, \"albert_config.json\")\n",
        "\n",
        "INIT_CHECKPOINT = tf.train.latest_checkpoint(ALBERT_GCS_DIR)\n",
        "\n",
        "albert_config = modeling.AlbertConfig.from_json_file(CONFIG_FILE)\n",
        "input_files = tf.gfile.Glob(os.path.join(DATA_GCS_DIR,'*tfrecord'))\n",
        "\n",
        "log.info(\"Using checkpoint: {}\".format(INIT_CHECKPOINT))\n",
        "log.info(\"Using {} data shards\".format(len(input_files)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQTFjITdd53F",
        "colab_type": "text"
      },
      "source": [
        "Prepare the training run configuration, build the estimator and input function, power up the bass cannon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMahsqUnZ55z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from run_pretraining import input_fn_builder, model_fn_builder\n",
        "\n",
        "\n",
        "model_fn = model_fn_builder(\n",
        "      albert_config=albert_config,\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      learning_rate=LEARNING_RATE,\n",
        "      num_train_steps=TRAIN_STEPS,\n",
        "      num_warmup_steps=3125,\n",
        "      use_tpu=USE_TPU,\n",
        "      optimizer=\"lamb\",\n",
        "      poly_power=1.0,\n",
        "      start_warmup_step=0,\n",
        "      use_one_hot_embeddings=USE_TPU)\n",
        "\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=ALBERT_GCS_DIR,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
        "        num_shards=NUM_TPU_CORES,\n",
        "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=USE_TPU,\n",
        "    model_fn=model_fn,\n",
        "    config=run_config,\n",
        "    train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE)\n",
        "  \n",
        "train_input_fn = input_fn_builder(\n",
        "        input_files=input_files,\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "        max_predictions_per_seq=MAX_PREDICTIONS,\n",
        "        is_training=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNt5ykopeIYB",
        "colab_type": "text"
      },
      "source": [
        "Start real pre-training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrCuEbr6dv8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator.train(input_fn=train_input_fn, max_steps=TRAIN_STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_OeXod-fHMT",
        "colab_type": "text"
      },
      "source": [
        "Training the model with the default parameters for 175k steps will take ~20 hours. \n",
        "\n",
        "In case the kernel is restarted, you may always continue training from the latest checkpoint. "
      ]
    }
  ]
}